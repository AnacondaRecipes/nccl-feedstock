{% set name = "nccl" %}
{% set version = "2.17.1-1" %}

package:
  name: {{ name|lower }}
  version: {{ version|replace("-", ".") }}

source:
  url: https://github.com/NVIDIA/nccl/archive/v{{ version }}.tar.gz
  sha256: 1311a6fd7cd44ad6d4523ba03065ce694605843fd30a5c0f77aa3d911abe706d
  patches:
    - remove-trace-call.patch
    # - makefile-debug.patch

build:
  number: 0
  skip: true  # [not linux]
  run_exports:
    # xref: https://github.com/NVIDIA/nccl/issues/218
    - {{ pin_subpackage(name, max_pin="x") }}

requirements:
  build:
    - {{ compiler("c") }}
    - {{ compiler("cxx") }}
    - {{ compiler("cuda") }}         # [cuda_compiler_version != "12.0"]
    - cuda-cudart ={{ cuda_compiler_version }}         # [cuda_compiler_version == "12.0"]
    - cuda-cudart-dev ={{ cuda_compiler_version }}     # [cuda_compiler_version == "12.0"]
    - cuda-cudart-static ={{ cuda_compiler_version }}  # [cuda_compiler_version == "12.0"]
    - cuda-nvcc ={{ cuda_compiler_version }}           # [cuda_compiler_version == "12.0"]
    - make
  host:
    - {{ compiler("c") }}
    - {{ compiler("cxx") }}
    - cuda-cudart ={{ cuda_compiler_version }}         # [cuda_compiler_version == "12.0"]
    - cuda-cudart-dev ={{ cuda_compiler_version }}     # [cuda_compiler_version == "12.0"]
    - cuda-cudart-static ={{ cuda_compiler_version }}  # [cuda_compiler_version == "12.0"]
    - cuda-nvcc ={{ cuda_compiler_version }}           # [cuda_compiler_version == "12.0"]

test:
  commands:
    - test -f "${PREFIX}/include/nccl.h"
    - test -f "${PREFIX}/lib/libnccl.so"
    - test ! -f "${PREFIX}/lib/libnccl_static.a"

about:
  home: https://developer.nvidia.com/nccl
  license: BSD-3-Clause
  license_family: BSD
  license_file: LICENSE.txt
  summary: Optimized primitives for collective multi-GPU communication

  description: |
    The NVIDIA Collective Communications Library (NCCL) implements multi-GPU
    and multi-node collective communication primitives that are performance
    optimized for NVIDIA GPUs. NCCL provides routines such as all-gather,
    all-reduce, broadcast, reduce, reduce-scatter, that are optimized to
    achieve high bandwidth over PCIe and NVLink high-speed interconnect.

  doc_url: https://docs.nvidia.com/deeplearning/sdk/nccl-developer-guide/docs/index.html
  dev_url: https://github.com/NVIDIA/nccl

extra:
  recipe-maintainers:
    - jakirkham
    - leofang
